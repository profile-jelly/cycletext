# import torch
# from tqdm import tqdm
# from transformers import T5Tokenizer, T5ForConditionalGeneration
# from datasets import load_dataset
# from torch.utils.data import DataLoader
# from evaluate import load
# from nltk.tokenize import word_tokenize
#
# # =========================================================
# # ğŸ”§ ç›´æ¥å†™æ­»çš„é…ç½®ï¼ˆå·²æŒ‰ä½ ç»™çš„è·¯å¾„å¡«å†™ï¼‰
# # =========================================================
#
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# TOKENIZER_PATH = r"G:\abnormal\cycletext\models\webnlg_t5_data2text_100"
#
# # ===== é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ =====
# # æ”¹è¿™é‡Œå³å¯åˆ‡æ¢æ¨¡å‹
# MODEL_PATH = r"G:\abnormal\cycletext\output\text2data-10"
# # MODEL_PATH = r"G:\abnormal\cycletext\output\data2text-10"
#
# # ===== å¯¹åº”æµ‹è¯•é›† =====
# TEST_FILE = r"G:\abnormal\cycletext\data\processed\webnlg-t5-unpaired\texts_unpaired.txt"
# # TEST_FILE = r"G:\abnormal\cycletext\data\processed\webnlg-t5-triplets2text\test.tsv"
#
# BATCH_SIZE = 8
# MAX_INPUT_LENGTH = 128
# MAX_OUTPUT_LENGTH = 128
# NUM_BEAMS = 1
#
# # =========================================================
# # ğŸš€ åŠ è½½æ¨¡å‹
# # =========================================================
#
# print("Loading model...")
# tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH)
# model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)
# model.to(DEVICE)
# model.eval()
#
# # =========================================================
# # ğŸ“„ åŠ è½½æµ‹è¯•æ•°æ®
# # =========================================================
#
# print("Loading test dataset...")
# dataset = load_dataset(
#     "csv",
#     data_files={"test": TEST_FILE},
#     delimiter="\t",
#     column_names=["source", "target"]
# )
#
# def tokenize(batch):
#     return tokenizer(
#         batch["source"],
#         padding="max_length",
#         truncation=True,
#         max_length=MAX_INPUT_LENGTH
#     )
#
# dataset = dataset.map(tokenize, batched=True)
# dataset.set_format(
#     type="torch",
#     columns=["input_ids", "attention_mask", "source", "target"]
# )
#
# loader = DataLoader(dataset["test"], batch_size=BATCH_SIZE)
#
# # =========================================================
# # ğŸ“Š æŒ‡æ ‡
# # =========================================================
#
# metric_bleu = load("bleu")
# metric_meteor = load("meteor")
# metric_rouge = load("rouge")
#
# predictions = []
# references = []
#
# # =========================================================
# # ğŸ” æ¨ç†
# # =========================================================
#
# print("Running inference...")
# with torch.no_grad():
#     for batch in tqdm(loader):
#         outputs = model.generate(
#             input_ids=batch["input_ids"].to(DEVICE),
#             attention_mask=batch["attention_mask"].to(DEVICE),
#             max_length=MAX_OUTPUT_LENGTH,
#             num_beams=NUM_BEAMS,
#             early_stopping=True
#         )
#
#         preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
#         predictions.extend(preds)
#         references.extend(batch["target"])
#
# # =========================================================
# # ğŸ§® è®¡ç®—æŒ‡æ ‡
# # =========================================================
#
# bleu = metric_bleu.compute(
#     predictions=[word_tokenize(p) for p in predictions],
#     references=[[word_tokenize(r)] for r in references]
# )["bleu"]
#
# meteor = metric_meteor.compute(
#     predictions=[" ".join(word_tokenize(p)) for p in predictions],
#     references=[[" ".join(word_tokenize(r))] for r in references]
# )["meteor"]
#
# rouge = metric_rouge.compute(predictions=predictions, references=references)
#
# # =========================================================
# # âœ… è¾“å‡ºç»“æœ
# # =========================================================
#
# print("\n================ Test Result ================")
# print(f"Model Path : {MODEL_PATH}")
# print(f"Test File  : {TEST_FILE}")
# print(f"BLEU       : {bleu:.4f}")
# print(f"METEOR     : {meteor:.4f}")
# print(f"ROUGE-L    : {rouge['rougeL'].mid.fmeasure:.4f}")
# print("============================================")
#
# # =========================================================
# # ğŸ’¾ ä¿å­˜ç”Ÿæˆç»“æœ
# # =========================================================
#
# out_path = MODEL_PATH + ".test.generations.txt"
# with open(out_path, "w", encoding="utf-8") as f:
#     for p in predictions:
#         f.write(p + "\n")
#
# print(f"\nGenerated outputs saved to:\n{out_path}")
import random
import random
import torch
from datasets import load_dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration
from nltk.tokenize import word_tokenize
from evaluate import load
from pathlib import Path

# ======================================================
# é…ç½®åŒº
# ======================================================
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# tokenizer æ°¸è¿œç”¨æœ€åˆçš„ t5-base
TOKENIZER_PATH = Path(r"G:/abnormal/cycletext/models/webnlg_t5_data2text_100")

# ===== é€‰æ‹©æ¨¡å‹ï¼ˆ2 é€‰ 1ï¼‰=====
MODEL_PATH = Path(r"G:/abnormal/cycletext/output/sb")
TASK_TYPE = "text2data"   # æˆ– "data2text"

# ===== æ•°æ®æ–‡ä»¶ =====
# pairedï¼ˆå¯ç®— BLEUï¼‰
# TEST_FILE = Path(r"G:/abnormal/cycletext/data/processed/webnlg-t5-triplets2text/test.tsv")

# unpairedï¼ˆåªèƒ½ sanity checkï¼‰
TEST_FILE = Path(r"G:/abnormal/cycletext/data/processed/webnlg-t5-unpaired/texts_unpaired.txt")

MAX_SAMPLES = 50
MAX_INPUT_LEN = 128
MAX_OUTPUT_LEN = 64
# ======================================================

print("Loading tokenizer & model...")
tokenizer = T5Tokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH, local_files_only=True).to(DEVICE)
model.eval()

print("Loading data...")

# ---------- æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½ ----------
if TEST_FILE.suffix == ".tsv":
    dataset = load_dataset(
        "csv",
        data_files={"test": str(TEST_FILE)},
        delimiter="\t",
        column_names=["source", "target"]
    )["test"]
    HAS_TARGET = True
else:
    dataset = load_dataset(
        "text",
        data_files={"test": str(TEST_FILE)}
    )["test"]
    HAS_TARGET = False

# éšæœºæŠ½æ ·
dataset = dataset.shuffle(seed=42).select(range(min(MAX_SAMPLES, len(dataset))))

if HAS_TARGET:
    bleu = load("bleu")
    predictions, references = [], []

print("\nRunning quick evaluation...\n")

used = 0
for i, sample in enumerate(dataset):

    if HAS_TARGET:
        source = sample["source"]
        target = sample["target"]

        if target is None or not isinstance(target, str) or target.strip() == "":
            continue
    else:
        source = sample["text"]
        target = None

    inputs = tokenizer(
        source,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=MAX_INPUT_LEN
    ).to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=MAX_OUTPUT_LEN,
            num_beams=1
        )

    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)
    used += 1

    # BLEUï¼ˆåªæœ‰ paired æ•°æ®æ‰ç®—ï¼‰
    if HAS_TARGET:
        predictions.append(word_tokenize(pred))
        references.append([word_tokenize(target)])

    # æ‰“å°å‰ 5 æ¡
    if used <= 5:
        print("=" * 80)
        print(f"[{used}] INPUT:")
        print(source)
        print("\nPRED:")
        print(pred)
        if HAS_TARGET:
            print("\nGOLD:")
            print(target)

# ---------- ç»“æœ ----------
print("\n" + "=" * 80)
print(f"Effective samples used: {used}")

if HAS_TARGET:
    bleu_score = bleu.compute(
        predictions=predictions,
        references=references
    )["bleu"]
    print(f"Quick BLEU: {bleu_score:.4f}")
else:
    print("Unpaired data â†’ qualitative sanity check only (no BLEU).")

print("=" * 80)


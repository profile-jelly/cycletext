# import os
# import requests
# from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
#
# def check_huggingface_connectivity():
#     """æ£€æµ‹æ˜¯å¦èƒ½è®¿é—® Hugging Face ä¸»ç«™"""
#     try:
#         r = requests.get("https://huggingface.co", timeout=5)
#         return r.status_code == 200
#     except Exception:
#         return False
#
#
# def load_or_download_model(model_name="t5-base", local_dir=r"G:\abnormal\cycletext\model"):
#     """
#     è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½æˆ–æç¤ºã€‚
#     model_name: æ¨¡å‹åæˆ–Hugging Faceè·¯å¾„ï¼Œä¾‹å¦‚ "t5-base"
#     local_dir: æŒ‡å®šæœ¬åœ°ç›®å½•ï¼Œå¦‚ r"G:\\models\\t5-base"
#     """
#     print(f"ğŸ” Checking model: {model_name}")
#
#     # è‹¥æŒ‡å®šæœ¬åœ°è·¯å¾„
#     if local_dir and os.path.exists(local_dir):
#         print(f"âœ… Found local model at: {local_dir}")
#         tokenizer = AutoTokenizer.from_pretrained(local_dir)
#         model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)
#         return tokenizer, model
#
#     # è‡ªåŠ¨æ£€æµ‹ç¼“å­˜
#     from transformers.utils import cached_file
#     try:
#         cached_path = cached_file(model_name, "config.json")
#         if cached_path:
#             print(f"âœ… Model already cached at: {os.path.dirname(cached_path)}")
#             tokenizer = AutoTokenizer.from_pretrained(model_name)
#             model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#             return tokenizer, model
#     except Exception:
#         pass
#
#     # è‹¥ç¼“å­˜ä¸å­˜åœ¨ï¼Œæ£€æµ‹ç½‘ç»œ
#     print("âš ï¸ Model not found locally, checking internet access...")
#     if check_huggingface_connectivity():
#         print("ğŸŒ Hugging Face reachable, downloading model...")
#         tokenizer = AutoTokenizer.from_pretrained(model_name)
#         model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
#         print("âœ… Download complete, model cached for future use.")
#         return tokenizer, model
#     else:
#         print("âŒ Cannot reach Hugging Face. Please manually download model from:")
#         print(f"   ğŸ‘‰ https://huggingface.co/{model_name}")
#         print("   Then place it under a local directory and re-run with:")
#         print('   load_or_download_model(local_dir=r"G:\\models\\t5-base")')
#         return None, None
#
#
# if __name__ == "__main__":
#     # ä¿®æ”¹ä¸ºä½ çš„ç›®æ ‡æ¨¡å‹
#     model_name = "t5-base"
#     local_path = r"G:\models\t5-base"
#
#     tokenizer, model = load_or_download_model(model_name, local_dir=local_path)
#
#     if model is not None:
#         print("\nâœ… Model loaded successfully!")
#         text = "Studies have shown that owning a dog is good for you"
#         inputs = tokenizer("summarize: " + text, return_tensors="pt")
#         outputs = model.generate(**inputs, max_new_tokens=30)
#         print("Generated text:", tokenizer.decode(outputs[0], skip_special_tokens=True))
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretradine(r"G:\models\t5-base")
model = T5ForConditionalGeneration.from_pretrained(r"G:\models\t5-base")

inputs = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=20)

print("âœ… Generated text:", tokenizer.decode(outputs[0], skip_special_tokens=True))
import os
# import shutil
# from pathlib import Path
#
# # æºè·¯å¾„ï¼ˆä½ æœ¬æœºç¼“å­˜çš„æ¨¡å‹ï¼‰
# src = Path(r"C:\Users\gh\.cache\huggingface\hub\models--t5-base\snapshots\a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1")
#
# # ç›®æ ‡è·¯å¾„
# dst = Path(r"G:\models\t5-base")
#
# print(f"ğŸ” Source: {src}")
# print(f"ğŸ“ Target: {dst}")
#
# if not src.exists():
#     raise FileNotFoundError(f"âŒ æºæ¨¡å‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {src}")
#
# dst.mkdir(parents=True, exist_ok=True)
#
# # æ‹·è´æ–‡ä»¶
# for file in src.iterdir():
#     if file.is_file():
#         shutil.copy(file, dst / file.name)
#         print(f"âœ… Copied: {file.name}")
#
# print("\nğŸ¯ æ¨¡å‹å·²æˆåŠŸå¤åˆ¶åˆ°æœ¬åœ°ç›®å½•ã€‚")
# print(f"â†’ æœ¬åœ°è·¯å¾„: {dst}")
# print("\nä½ ç°åœ¨å¯ä»¥åœ¨ä»£ç ä¸­ä½¿ç”¨ä»¥ä¸‹è·¯å¾„åŠ è½½æ¨¡å‹ï¼š\n")
# print(f'  from transformers import T5Tokenizer, T5ForConditionalGeneration')
# print(f'  tokenizer = T5Tokenizer.from_pretrained(r"{dst}")')
# print(f'  model = T5ForConditionalGeneration.from_pretrained(r"{dst}")')

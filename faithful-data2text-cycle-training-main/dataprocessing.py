# import os
# import xml.etree.ElementTree as ET
#
# # === è¾“å…¥ä¸è¾“å‡ºè·¯å¾„ ===
# root_dir = r"G:\abnormal\cycletext\faithful-data2text-cycle-training-main\faithful-data2text-cycle-training-main\datasets\webnlg-dataset-master\webnlg-dataset-master\release_v3.0\en"
# output_base = r"G:\abnormal\cycletext\data"
#
# splits = ["train", "dev", "test"]
#
# # faithful-data2text-cycle-training æ‰€éœ€è¾“å‡ºè·¯å¾„
# out_trip2text = os.path.join(output_base, "webnlg-t5-triplets2text")
# out_text2trip = os.path.join(output_base, "webnlg-t5-text2triplets")
# os.makedirs(out_trip2text, exist_ok=True)
# os.makedirs(out_text2trip, exist_ok=True)
#
#
# def extract_entries(xml_path, is_test=False):
#     """ä» WebNLG v3.0 XML æ–‡ä»¶ä¸­æå– (triples, text) å¯¹"""
#     pairs = []
#     try:
#         tree = ET.parse(xml_path)
#         root = tree.getroot()
#
#         for entry in root.iter("entry"):
#             triples = []
#             # æ”¯æŒ modifiedtripleset, originaltripleset, tripleset
#             for node_name in ["modifiedtripleset", "originaltripleset", "tripleset"]:
#                 for tag in ["mtriple", "otriple", "triple"]:
#                     for triple in entry.findall(f".//{node_name}/{tag}"):
#                         if triple.text:
#                             triples.append(triple.text.strip())
#
#             if not triples:
#                 continue
#             triple_str = " && ".join(triples)
#
#             # æå–è‡ªç„¶è¯­è¨€å¥å­ï¼ˆæ–°ç‰ˆæ˜¯ <lex>ï¼‰
#             texts = []
#             for lex in entry.findall(".//lex"):
#                 if lex.text:
#                     text = lex.text.strip()
#                     if text:
#                         texts.append(text)
#
#             if texts:
#                 for text in texts:
#                     pairs.append((triple_str, text))
#             else:
#                 # æµ‹è¯•é›†æ²¡æœ‰æ–‡æœ¬æ—¶ä¿ç•™ tripleï¼Œtext è®¾ä¸º None
#                 if is_test:
#                     pairs.append((triple_str, None))
#     except Exception as e:
#         print(f"[Error parsing] {xml_path}: {e}")
#     return pairs
#
#
# for split in splits:
#     print(f"\nğŸ“‚ Processing {split}...")
#     split_dir = os.path.join(root_dir, split)
#     all_pairs = []
#     is_test = split == "test"
#
#     # === éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹ (1triples, 2triples...) ===
#     for subdir, _, files in os.walk(split_dir):
#         for fname in files:
#             if fname.endswith(".xml"):
#                 xml_path = os.path.join(subdir, fname)
#                 pairs = extract_entries(xml_path, is_test=is_test)
#                 if len(pairs) == 0:
#                     print(f"âš ï¸ no pairs extracted from {xml_path}")
#                 else:
#                     print(f"âœ… {len(pairs)} pairs extracted from {os.path.relpath(xml_path, split_dir)}")
#                 all_pairs.extend(pairs)
#
#     print(f"â†’ Total extracted {len(all_pairs)} pairs for {split}")
#
#     # === è¾“å‡º triplets2text (ä¸‰å…ƒç»„â†’æ–‡æœ¬) ===
#     data2text_source = [f"Generate in English: {t}" for t, _ in all_pairs]
#     data2text_tsv = [f"Generate in English: {t}\t{s}" for t, s in all_pairs if s is not None]
#
#     split_name = "train" if split == "train" else "val" if split == "dev" else "test"
#
#     with open(os.path.join(out_trip2text, f"{split_name}.source"), "w", encoding="utf-8") as f:
#         f.write("\n".join(data2text_source))
#     with open(os.path.join(out_trip2text, f"{split_name}.tsv"), "w", encoding="utf-8") as f:
#         f.write("\n".join(data2text_tsv))
#
#     # === è¾“å‡º text2triplets (æ–‡æœ¬â†’ä¸‰å…ƒç»„ï¼Œä»… train/dev) ===
#     if not is_test:
#         text2data_source = [f"Extract Triplets: {s}" for _, s in all_pairs]
#         text2data_tsv = [f"Extract Triplets: {s}\t{t}" for t, s in all_pairs]
#
#         with open(os.path.join(out_text2trip, f"{split_name}.source"), "w", encoding="utf-8") as f:
#             f.write("\n".join(text2data_source))
#         with open(os.path.join(out_text2trip, f"{split_name}.tsv"), "w", encoding="utf-8") as f:
#             f.write("\n".join(text2data_tsv))
#
# print("\nâœ… æ•°æ®é›†æå–ä¸æ ¼å¼è½¬æ¢å®Œæˆï¼Œå¯ç›´æ¥ç”¨äº faithful-data2text-cycle-training é¡¹ç›®ï¼")
# import os
# import random
# import glob
# import xml.etree.ElementTree as ET
# from tqdm import tqdm
#
#
# def extract_triples_from_xml(xml_path):
#     """ä» WebNLG XML æ–‡ä»¶ä¸­æå– (triples, text) å¯¹"""
#     pairs = []
#     try:
#         tree = ET.parse(xml_path)
#         root = tree.getroot()
#     except Exception as e:
#         print(f"âš ï¸ æ— æ³•è§£æ {xml_path}: {e}")
#         return pairs
#
#     # å¯»æ‰¾æ‰€æœ‰ entry èŠ‚ç‚¹ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
#     entries = root.findall(".//entry")
#     if not entries:
#         entries = root.findall(".//benchmark/entries/entry")
#
#     for entry in entries:
#         triples = []
#         for mtriple in entry.findall(".//modifiedtripleset/mtriple"):
#             if mtriple.text:
#                 triples.append(mtriple.text.strip())
#         triple_str = " | ".join(triples)
#         if not triple_str:
#             continue
#
#         for lex in entry.findall(".//lex"):
#             if lex.text:
#                 text = lex.text.strip().replace("\n", " ")
#                 pairs.append((triple_str, text))
#
#     return pairs
#
#
# def process_webnlg_dataset(base_dir, output_dir):
#     """ä¸»å¤„ç†é€»è¾‘ï¼šç”Ÿæˆ fullã€100-pairedã€unpaired ä¸‰å¥—æ•°æ®"""
#     splits = ["train", "dev", "test"]
#     all_pairs = []
#
#     os.makedirs(output_dir, exist_ok=True)
#
#     print("ğŸ” å¼€å§‹è§£æ WebNLG æ•°æ®é›†...")
#     for split in splits:
#         # WebNLG v3.0 å®é™…ç›®å½•ï¼šrelease_v3.0/en/train/1triples/*.xml
#         xml_dir = os.path.join(base_dir, "en", split)
#         print(f"\nğŸ“‚ å½“å‰å¤„ç†è·¯å¾„: {xml_dir}")
#
#         xml_files = glob.glob(os.path.join(xml_dir, "**/*.xml"), recursive=True)
#         print(f"å…±æ‰¾åˆ° {len(xml_files)} ä¸ª XML æ–‡ä»¶ã€‚")
#
#         split_pairs = []
#         for xml_file in tqdm(xml_files, desc=f"Processing {split}"):
#             split_pairs.extend(extract_triples_from_xml(xml_file))
#
#         # ä¿å­˜å®Œæ•´pairedæ•°æ®
#         split_dir_data2text = os.path.join(output_dir, f"webnlg-t5-triplets2text/{split}")
#         split_dir_text2data = os.path.join(output_dir, f"webnlg-t5-text2triplets/{split}")
#         os.makedirs(split_dir_data2text, exist_ok=True)
#         os.makedirs(split_dir_text2data, exist_ok=True)
#
#         data2text_path = os.path.join(split_dir_data2text, f"{split}.tsv")
#         text2data_path = os.path.join(split_dir_text2data, f"{split}.tsv")
#
#         with open(data2text_path, "w", encoding="utf-8") as f1, open(text2data_path, "w", encoding="utf-8") as f2:
#             for triple, text in split_pairs:
#                 f1.write(f"Generate in English: {triple}\t{text}\n")
#                 f2.write(f"Extract Triplets: {text}\t{triple}\n")
#
#         print(f"âœ… {split} å®Œæ•´æ•°æ®ä¿å­˜å®Œæˆï¼Œå…± {len(split_pairs)} å¯¹æ ·æœ¬ã€‚")
#
#         if split == "train":
#             all_pairs.extend(split_pairs)
#
#     # === æ„å»ºä½èµ„æºç‰ˆæœ¬ ===
#     print("\nğŸ¯ å¼€å§‹æ„å»ºä½èµ„æºç‰ˆæœ¬ï¼ˆ100æ¡ + unpairedï¼‰...")
#     random.seed(42)
#     random.shuffle(all_pairs)
#
#     paired_100 = all_pairs[:100]
#     unpaired_rest = all_pairs[100:]
#
#     # 100æ¡ paired
#     paired_dir = os.path.join(output_dir, "webnlg-t5-100paired")
#     os.makedirs(paired_dir, exist_ok=True)
#     with open(os.path.join(paired_dir, "train_data2text.tsv"), "w", encoding="utf-8") as f:
#         for t, s in paired_100:
#             f.write(f"Generate in English: {t}\t{s}\n")
#     with open(os.path.join(paired_dir, "train_text2data.tsv"), "w", encoding="utf-8") as f:
#         for t, s in paired_100:
#             f.write(f"Extract Triplets: {s}\t{t}\n")
#     print(f"âœ… å·²ç”Ÿæˆ 100 æ¡ paired æ•°æ®ã€‚")
#
#     # å‰©ä½™ unpaired
#     unpaired_dir = os.path.join(output_dir, "webnlg-t5-unpaired")
#     os.makedirs(unpaired_dir, exist_ok=True)
#     triples_unpaired = [f"Generate in English: {t}" for t, _ in unpaired_rest]
#     texts_unpaired = [f"Extract Triplets: {s}" for _, s in unpaired_rest]
#
#     with open(os.path.join(unpaired_dir, "triples_unpaired.txt"), "w", encoding="utf-8") as f:
#         f.write("\n".join(triples_unpaired))
#     with open(os.path.join(unpaired_dir, "texts_unpaired.txt"), "w", encoding="utf-8") as f:
#         f.write("\n".join(texts_unpaired))
#
#     print(f"âœ… å·²ç”Ÿæˆ unpaired æ•°æ®ï¼š{len(unpaired_rest)} æ¡ã€‚")
#
#     print("\nğŸ‰ æ•°æ®æ„å»ºå®Œæˆï¼è¾“å‡ºç»“æ„å¦‚ä¸‹ï¼š")
#     print(f"{output_dir}/")
#     print("â”œâ”€â”€ webnlg-t5-triplets2text/ (å®Œæ•´è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†)")
#     print("â”œâ”€â”€ webnlg-t5-text2triplets/ (å®Œæ•´è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†)")
#     print("â”œâ”€â”€ webnlg-t5-100paired/ (100æ¡æœ‰æ ‡æ³¨æ ·æœ¬)")
#     print("â””â”€â”€ webnlg-t5-unpaired/ (å‰©ä½™æ— æ ‡æ³¨æ•°æ®)")
#     print("âœ… å¯ç›´æ¥ç”¨äºåŸºçº¿æ¨¡å‹å’Œ CycleText æ¨¡å‹è®­ç»ƒã€‚")
#
#
# if __name__ == "__main__":
#     import argparse
#     parser = argparse.ArgumentParser(description="Convert WebNLG XML to CycleText format")
#     parser.add_argument("--webnlg_dir", type=str, required=True,
#                         help="WebNLG æ•°æ®é›†çš„æ ¹ç›®å½•ï¼ˆåŒ…å« release_v3.0/en/train/...ï¼‰")
#     parser.add_argument("--output_dir", type=str, required=True,
#                         help="è¾“å‡ºè·¯å¾„ï¼ˆå°†ç”Ÿæˆå¤šä¸ªå­æ–‡ä»¶å¤¹ï¼‰")
#     args = parser.parse_args()
#
#     process_webnlg_dataset(args.webnlg_dir, args.output_dir)
import os
import random
import glob
import xml.etree.ElementTree as ET
from tqdm import tqdm


def extract_triples_from_xml(xml_path):
    """ä» WebNLG XML æ–‡ä»¶ä¸­æå– (triples, text) å¯¹"""
    pairs = []
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è§£æ {xml_path}: {e}")
        return pairs

    entries = root.findall(".//entry")
    if not entries:
        entries = root.findall(".//benchmark/entries/entry")

    for entry in entries:
        triples = []
        for mtriple in entry.findall(".//modifiedtripleset/mtriple"):
            if mtriple.text:
                triples.append(mtriple.text.strip())
        triple_str = " | ".join(triples)
        if not triple_str:
            continue

        for lex in entry.findall(".//lex"):
            if lex.text:
                text = lex.text.strip().replace("\n", " ")
                pairs.append((triple_str, text))

    return pairs


def process_webnlg_dataset(base_dir, output_dir):
    """ç”Ÿæˆ fullã€100-pairedã€unpaired ä¸‰å¥—æ•°æ® (å« .tsv, .source, .target)"""
    splits = ["train", "dev", "test"]
    all_pairs = []

    os.makedirs(output_dir, exist_ok=True)

    print("ğŸ” å¼€å§‹è§£æ WebNLG æ•°æ®é›†...")
    for split in splits:
        xml_dir = os.path.join(base_dir, "en", split)
        print(f"\nğŸ“‚ å½“å‰å¤„ç†è·¯å¾„: {xml_dir}")

        xml_files = glob.glob(os.path.join(xml_dir, "**/*.xml"), recursive=True)
        print(f"å…±æ‰¾åˆ° {len(xml_files)} ä¸ª XML æ–‡ä»¶ã€‚")

        split_pairs = []
        for xml_file in tqdm(xml_files, desc=f"Processing {split}"):
            split_pairs.extend(extract_triples_from_xml(xml_file))

        # === ä¿å­˜å®Œæ•´ paired æ•°æ® ===
        split_dir_data2text = os.path.join(output_dir, f"webnlg-t5-triplets2text/{split}")
        split_dir_text2data = os.path.join(output_dir, f"webnlg-t5-text2triplets/{split}")
        os.makedirs(split_dir_data2text, exist_ok=True)
        os.makedirs(split_dir_text2data, exist_ok=True)

        data2text_tsv = os.path.join(split_dir_data2text, f"{split}.tsv")
        text2data_tsv = os.path.join(split_dir_text2data, f"{split}.tsv")
        data2text_source = os.path.join(split_dir_data2text, f"{split}.source")
        data2text_target = os.path.join(split_dir_data2text, f"{split}.target")
        text2data_source = os.path.join(split_dir_text2data, f"{split}.source")
        text2data_target = os.path.join(split_dir_text2data, f"{split}.target")

        with open(data2text_tsv, "w", encoding="utf-8") as f1, \
             open(text2data_tsv, "w", encoding="utf-8") as f2, \
             open(data2text_source, "w", encoding="utf-8") as s1, \
             open(data2text_target, "w", encoding="utf-8") as t1, \
             open(text2data_source, "w", encoding="utf-8") as s2, \
             open(text2data_target, "w", encoding="utf-8") as t2:

            for triple, text in split_pairs:
                # æ–¹å‘1: triples -> text
                f1.write(f"Generate in English: {triple}\t{text}\n")
                s1.write(f"Generate in English: {triple}\n")
                t1.write(f"{text}\n")
                # æ–¹å‘2: text -> triples
                f2.write(f"Extract Triplets: {text}\t{triple}\n")
                s2.write(f"Extract Triplets: {text}\n")
                t2.write(f"{triple}\n")

        print(f"âœ… {split} å®Œæ•´æ•°æ®ä¿å­˜å®Œæˆï¼Œå…± {len(split_pairs)} å¯¹æ ·æœ¬ã€‚")

        if split == "train":
            all_pairs.extend(split_pairs)

    # === æ„å»ºä½èµ„æºç‰ˆæœ¬ ===
    print("\nğŸ¯ å¼€å§‹æ„å»ºä½èµ„æºç‰ˆæœ¬ï¼ˆ100æ¡ + unpairedï¼‰...")
    random.seed(42)
    random.shuffle(all_pairs)

    paired_100 = all_pairs[:100]
    unpaired_rest = all_pairs[100:]

    # 100 æ¡ paired
    paired_dir = os.path.join(output_dir, "webnlg-t5-100paired")
    os.makedirs(paired_dir, exist_ok=True)
    with open(os.path.join(paired_dir, "train_data2text.tsv"), "w", encoding="utf-8") as f1, \
         open(os.path.join(paired_dir, "train_text2data.tsv"), "w", encoding="utf-8") as f2, \
         open(os.path.join(paired_dir, "train_data2text.source"), "w", encoding="utf-8") as s1, \
         open(os.path.join(paired_dir, "train_data2text.target"), "w", encoding="utf-8") as t1, \
         open(os.path.join(paired_dir, "train_text2data.source"), "w", encoding="utf-8") as s2, \
         open(os.path.join(paired_dir, "train_text2data.target"), "w", encoding="utf-8") as t2:

        for t, s in paired_100:
            f1.write(f"Generate in English: {t}\t{s}\n")
            f2.write(f"Extract Triplets: {s}\t{t}\n")
            s1.write(f"Generate in English: {t}\n")
            t1.write(f"{s}\n")
            s2.write(f"Extract Triplets: {s}\n")
            t2.write(f"{t}\n")

    print(f"âœ… å·²ç”Ÿæˆ 100 æ¡ paired æ•°æ®ã€‚")

    # å‰©ä½™ unpaired
    unpaired_dir = os.path.join(output_dir, "webnlg-t5-unpaired")
    os.makedirs(unpaired_dir, exist_ok=True)
    triples_unpaired = [f"Generate in English: {t}" for t, _ in unpaired_rest]
    texts_unpaired = [f"Extract Triplets: {s}" for _, s in unpaired_rest]

    with open(os.path.join(unpaired_dir, "triples_unpaired.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(triples_unpaired))
    with open(os.path.join(unpaired_dir, "texts_unpaired.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(texts_unpaired))

    print(f"âœ… å·²ç”Ÿæˆ unpaired æ•°æ®ï¼š{len(unpaired_rest)} æ¡ã€‚")

    print("\nğŸ‰ æ•°æ®æ„å»ºå®Œæˆï¼è¾“å‡ºç»“æ„å¦‚ä¸‹ï¼š")
    print(f"{output_dir}/")
    print("â”œâ”€â”€ webnlg-t5-triplets2text/ (å®Œæ•´è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†, å« .source/.target)")
    print("â”œâ”€â”€ webnlg-t5-text2triplets/ (å®Œæ•´è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†, å« .source/.target)")
    print("â”œâ”€â”€ webnlg-t5-100paired/ (100æ¡æœ‰æ ‡æ³¨æ ·æœ¬, å« .source/.target)")
    print("â””â”€â”€ webnlg-t5-unpaired/ (å‰©ä½™æ— æ ‡æ³¨æ•°æ®)")
    print("âœ… å¯ç›´æ¥ç”¨äºåŸºçº¿æ¨¡å‹å’Œ CycleText æ¨¡å‹è®­ç»ƒã€‚")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Convert WebNLG XML to CycleText format (.source/.target included)")
    parser.add_argument("--webnlg_dir", type=str, required=True,
                        help="WebNLG æ•°æ®é›†æ ¹ç›®å½•ï¼ˆåŒ…å« release_v3.0/en/train/...ï¼‰")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="è¾“å‡ºè·¯å¾„ï¼ˆå°†ç”Ÿæˆå¤šä¸ªå­æ–‡ä»¶å¤¹ï¼‰")
    args = parser.parse_args()

    process_webnlg_dataset(args.webnlg_dir, args.output_dir)

